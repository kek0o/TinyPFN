"""Data loading utilities for tabular priors."""

from typing import Callable, Dict, Iterator, Union

import h5py
import torch
from torch.utils.data import DataLoader

from .dataset import PriorDataset


class PriorDataLoader(DataLoader):
    """Generic DataLoader for synthetic data generation using a get_batch function."""

    def __init__(
        self,
        get_batch_function: Callable[..., Dict[str, Union[torch.Tensor, int]]],
        num_steps: int,
        batch_size: int,
        num_datapoints_max: int,
        num_features: int,
        device: torch.device,
    ):
        self.get_batch_function = get_batch_function
        self.num_steps = num_steps
        self.batch_size = batch_size
        self.num_datapoints_max = num_datapoints_max
        self.num_features = num_features
        self.device = device

    def __iter__(self) -> Iterator[Dict[str, Union[torch.Tensor, int]]]:
        return iter(
            self.get_batch_function(self.batch_size, self.num_datapoints_max, self.num_features)
            for _ in range(self.num_steps)
        )

    def __len__(self) -> int:
        return self.num_steps


class PriorDumpDataLoader(DataLoader):
    """DataLoader that loads synthetic prior data from an HDF5 dump."""
    
    def __init__(self, filename, num_steps, batch_size, device, starting_index=0):
        self.filename = filename
        self.num_steps = num_steps
        self.batch_size = batch_size
        with h5py.File(self.filename, "r") as f:
            self.num_datapoints_max = f['X'].shape[0]
            if "max_num_classes" in f:
                self.max_num_classes = f["max_num_classes"][0]
            else:
                self.max_num_classes = None
            self.problem_type = f["problem_type"][()].decode("utf-8")
            self.has_num_datapoints = "num_datapoints" in f
            self.stored_max_seq_len = f["X"].shape[1]
        self.device = device
        self.pointer = starting_index

    def __iter__(self):
        with h5py.File(self.filename, "r") as f:
            for _ in range(self.num_steps):
                end = self.pointer + self.batch_size

                num_features = f["num_features"][self.pointer : end].max()
                if self.has_num_datapoints:
                    num_datapoints_batch = f["num_datapoints"][self.pointer:end]
                    max_seq_in_batch = int(num_datapoints_batch.max())
                else:
                    max_seq_in_batch = int(self.stored_max_seq_len)

                x = torch.from_numpy(f["X"][self.pointer:end, :max_seq_in_batch, :num_features])
                y = torch.from_numpy(f["y"][self.pointer:end, :max_seq_in_batch])
                single_eval_pos = f["single_eval_pos"][self.pointer : end]

                self.pointer += self.batch_size
                if self.pointer >= f["X"].shape[0]:
                    print(
                        """Finished iteration over all stored datasets! """
                        """Will start reusing the same data with different splits now."""
                    )
                    self.pointer = 0

                yield dict(
                    x=x.to(self.device),
                    y=y.to(self.device),
                    target_y=y.to(self.device),
                    single_eval_pos=single_eval_pos[0].item(),
                )

    def __len__(self):
        return self.num_steps


class LocalPriorDataLoader(DataLoader):
    """DataLoader sampling synthetic prior data on-the-fly from our local PriorDataset.
    
    This replaces TabICLPriorDataLoader and adds min_classes support.
    """

    def __init__(
        self,
        num_steps: int,
        batch_size: int,
        num_datapoints_min: int,
        num_datapoints_max: int,
        min_features: int,
        max_features: int,
        min_num_classes: int,
        max_num_classes: int,
        device: torch.device,
    ):
        self.num_steps = num_steps
        self.batch_size = batch_size
        self.num_datapoints_min = num_datapoints_min
        self.num_datapoints_max = num_datapoints_max
        self.min_features = min_features
        self.max_features = max_features
        self.min_num_classes = min_num_classes
        self.max_num_classes = max_num_classes
        self.device = device

        self.pd = PriorDataset(
            batch_size=batch_size,
            batch_size_per_gp=batch_size,
            min_features=min_features,
            max_features=max_features,
            min_classes=min_num_classes,
            max_classes=max_num_classes,
            min_seq_len=num_datapoints_min,
            max_seq_len=num_datapoints_max,
        )

    def _convert_batch(self, d):
        x, y, active_features, seqlen, train_size = d
        active_features = active_features[0].item()
        x = x[:, :, :active_features]
        single_eval_pos = train_size[0].item()
        return dict(
            x=x.to(self.device),
            y=y.to(self.device),
            target_y=y.to(self.device),
            single_eval_pos=single_eval_pos,
        )

    def __iter__(self):
        return iter(self._convert_batch(next(self.pd)) for _ in range(self.num_steps))

    def __len__(self):
        return self.num_steps
